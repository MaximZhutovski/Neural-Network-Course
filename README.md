## Deep Learning Projects Overview
This repository showcases two deep learning projects I completed as part of my academic work. Each project applies core deep learning techniques to a real-world problem, demonstrating my understanding of neural network design, training procedures, and evaluation strategies in both computer vision and natural language processing.

# Project 1: CNN for Handwritten Digit Classification (MNIST)
Goal: Classify handwritten digits (0‚Äì9) using various neural network architectures
Techniques Used:
Implemented and compared multiple models:
Fully connected networks
Convolutional Neural Networks (CNNs)
CNNs with dropout layers
Trained and evaluated models using accuracy, F1-score, precision, recall, and balanced accuracy.
Visualized learned convolutional filters and feature maps.
Experimented with different batch sizes to examine training efficiency and convergence.
Frameworks: PyTorch, scikit-learn, matplotlib

# Project 2: LSTM Language Modeling on Simple Wikipedia Text
Goal: Train a language model to predict sequences of words using a simplified English corpus.
Techniques Used:
Built a vocabulary of the top 10,000 words from the dataset.
Designed and trained an LSTM-based neural network using Keras (via TensorFlow).
Used Embedding, LSTM, and TimeDistributed(Dense) layers to model word sequences.
Configured custom training loops for forward and backward direction text prediction.
Managed tokenization, padding, batching, and evaluation of model performance.
Integrated TensorBoardX for monitoring training progress.
Frameworks: TensorFlow, Keras, NumPy, scikit-learn, tensorboardX

# Core Competencies Highlighted
üß† Model Design: Built and compared CNN and LSTM architectures for vision and NLP tasks.
‚öôÔ∏è Frameworks: Practical experience with both PyTorch and TensorFlow/Keras.
üßπ Data Preprocessing: Handled batching, padding, tokenization, and normalization.
üìä Evaluation: Used metrics like Accuracy, F1, Precision, Recall, and Balanced Accuracy.
üîç Visualization: Visualized filters, activations, and model behavior.
üöÄ Experimentation: Explored batch sizes, depth, and training strategies.
üìÅ Clean Code: Modular, well-structured, and easy to reproduce.

# What‚Äôs Next
I plan to continue expanding this repository with additional projects as I deepen my knowledge and skills in deep learning. My goal is to gain hands-on experience with advanced architectures, explore a variety of tasks in computer vision and natural language processing, and build a solid foundation for research and applied work in the field.









