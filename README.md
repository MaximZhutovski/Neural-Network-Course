# Deep Learning Projects Overview
This repository showcases two deep learning projects I completed as part of my academic work. Each project applies core deep learning techniques to a real-world 
problem, demonstrating my understanding of neural network design, training procedures, and evaluation strategies in both computer vision and natural language processing. 

## Project 1: XOR Neural Network (First Project)  
**Goal:** Build a basic neural network from scratch to solve the XOR problem using PyTorch.  
**Techniques Used:**  
1.Manually implemented linear layer and sigmoid activation with temperature control.  
2.Constructed a simple MLP architecture with optional bypass connections.  
3.Trained the model to learn the XOR function using binary inputs.  
4.Focused on understanding the limitations of linear models and importance of non-linearity.  
5.Frameworks: PyTorch  

## Project 2: XOR Neural Network (Extended Model)  
**Goal:** Create a flexible, modular MLP architecture to solve the XOR problem with configurable design.  
**Techniques Used:**  
1.Defined dynamic hidden layer size and optional input-output bypass connections.   
2.Used a custom BTU activation function with tunable temperature.  
3.Refactored the model into modular classes for reusability and clarity.  
4.Added GPU support and configurable parameter initialization.  
5.Frameworks: PyTorch  


## Project 3: CNN for Handwritten Digit Classification (MNIST)  
**Goal:** Classify handwritten digits (0‚Äì9) using various neural network architectures  
**Techniques Used:**  
Implemented and compared multiple models:  
1.Fully connected networks  
2.Convolutional Neural Networks (CNNs)  
3.CNNs with dropout layers  
4.Trained and evaluated models using accuracy, F1-score, precision, recall, and balanced accuracy.  
5.Visualized learned convolutional filters and feature maps.  
6.Experimented with different batch sizes to examine training efficiency and convergence.  
7.Frameworks: PyTorch, scikit-learn, matplotlib  

## Project 4: LSTM Language Modeling on Simple Wikipedia Text  
**Goal:** Train a language model to predict sequences of words using a simplified English corpus.  
**Techniques Used:**  
1.Built a vocabulary of the top 10,000 words from the dataset.  
2.Designed and trained an LSTM-based neural network using Keras (via TensorFlow).  
3.Used Embedding, LSTM, and TimeDistributed(Dense) layers to model word sequences.  
4.Configured custom training loops for forward and backward direction text prediction.  
5.Managed tokenization, padding, batching, and evaluation of model performance.  
6.Integrated TensorBoardX for monitoring training progress.  
7.Frameworks: TensorFlow, Keras, NumPy, scikit-learn, tensorboardX  

## Core Competencies Highlighted
üß† Model Design: Built and compared CNN, LSTM, and MLP architectures for vision and NLP tasks.  
‚öôÔ∏è Frameworks: Practical experience with both PyTorch and TensorFlow/Keras.  
üîß Low-Level Implementation: Manually implemented linear layers, activation functions, and forward passes in PyTorch.  
üñ•Ô∏è Device Optimization: Integrated GPU support for efficient model training.  
üßπ Data Preprocessing: Handled batching, padding, tokenization, and normalization.  
üìä Evaluation: Used metrics like Accuracy, F1, Precision, Recall, and Balanced Accuracy.  
üîç Visualization: Visualized filters, activations, and model behavior.  
üöÄ Experimentation: Explored batch sizes, architecture depth, and training strategies.  
üìÅ Clean Code: Modular, well-structured, and easy to reproduce.  


## What‚Äôs Next
I plan to continue expanding this repository with additional projects as I deepen my knowledge and skills in deep learning. My goal is to gain hands-on experience with advanced architectures, explore a variety of tasks in computer vision and natural language processing, and build a solid foundation for research and applied work in the field.









