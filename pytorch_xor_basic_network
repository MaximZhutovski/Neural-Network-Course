{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWC7bOJ0cpil"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import math\n",
        "from IPython import display\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XOR\n",
        "# 0 0 -> 0\n",
        "# 0 1 -> 1\n",
        "# 1 0 -> 1\n",
        "# 1 1 -> 0"
      ],
      "metadata": {
        "id": "QT7JreUmcdH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid (z, temp = 0.001):\n",
        "  return 1 / (1 + torch.exp(-z/temp))\n"
      ],
      "metadata": {
        "id": "lSPZYkrycsvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BTU(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, T=0.001, inplace: bool = False):\n",
        "      super(BTU, self).__init__()\n",
        "      self.T = T\n",
        "\n",
        "  def forward(self, input: torch.Tensor):\n",
        "      return sigmoid(input)"
      ],
      "metadata": {
        "id": "H51ugvXOdLIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(torch.nn.Module):\n",
        "    def __init__(self, input_feat: int, output_feat: int, bias: bool = True, device=None, dtype=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super(Linear, self).__init__()\n",
        "        self.input_feat = input_feat\n",
        "        self.output_feat = output_feat\n",
        "        self.weight = nn.Parameter(torch.empty((output_feat, input_feat), **factory_kwargs))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.empty(output_feat, **factory_kwargs))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "    def set_weights(self, w: torch.Tensor, b: torch.Tensor):\n",
        "        if w.shape != self.weight.shape or (b is not None and b.shape != self.bias.shape):\n",
        "            raise ValueError(\"Weight and bias shapes do not match the layer dimensions.\")\n",
        "        self.weight = nn.Parameter(w)\n",
        "        if self.bias is not None:\n",
        "            self.bias = nn.Parameter(b)\n",
        "\n",
        "    def forward(self, input: torch.Tensor):\n",
        "        return torch.matmul(input, self.weight.T) + self.bias\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return 'input_feat={}, output_feat={}, bias={}'.format(\n",
        "            self.input_feat, self.output_feat, self.bias is not None)\n",
        "\n"
      ],
      "metadata": {
        "id": "dW7VVMp7WSYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 2\n",
        "output_dim = 1\n",
        "\n",
        "class Network(nn.Module):\n",
        "  def __init__(self, k, bypass = False):\n",
        "    super().__init__()\n",
        "    self.hidden = Linear(input_dim, k)\n",
        "    self.bypass = bypass\n",
        "    if self.bypass:\n",
        "      self.output = Linear(k + input_dim, output_dim)\n",
        "    else:\n",
        "      self.output = Linear(k, output_dim)\n",
        "    self.BTU = BTU()\n",
        "\n",
        "  def set_weights(self, w, b, layer):\n",
        "      if layer=='hidden':\n",
        "        if w.shape==self.hidden.weight.shape and b.shape==self.hidden.bias.shape:\n",
        "          self.hidden.set_weights(w,b)\n",
        "        else:\n",
        "          print(\"Input matrix size differs from hidden layer initialization matrix size\")\n",
        "      if layer=='output':\n",
        "        if w.shape==self.output.weight.shape and b.shape==self.output.bias.shape:\n",
        "          self.output.set_weights(w,b)\n",
        "        else:\n",
        "          print(\"Input matrix size differs from output layer initialization matrix size\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, input):\n",
        "    z1 = self.hidden(input)\n",
        "    y1 = self.BTU(z1)\n",
        "    if self.bypass:\n",
        "      y1_concat = torch.cat((input, y1), 1)\n",
        "      #print(y1_concat)\n",
        "      z2 = self.output(y1_concat)\n",
        "    else:\n",
        "      z2 = self.output(y1)\n",
        "    return self.BTU(z2)\n",
        "\n"
      ],
      "metadata": {
        "id": "LB3rs_0leKhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Loss(x, t, print_deltas=False):\n",
        "  squared_deltas = torch.square(model(x) - t) ## SSE\n",
        "  if print_deltas:\n",
        "    print(squared_deltas)\n",
        "  return torch.sum(squared_deltas)"
      ],
      "metadata": {
        "id": "TIlGf-NIsPri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For K=1, a bypass is necessary for the model to function correctly because XOR is a non-linear function, and a single BTU can only handle linear functions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bJasTeyxa28M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k=1 #the number of neurons in the hidden layer\n",
        "\n",
        "model=Network(k,True)\n",
        "\n",
        "w_hidden = torch.tensor([[1.0, 1.0]]) #The weights of the neurons in the hidden layer\n",
        "b_hidden = torch.tensor([-1.5]) #The weight of the bias in the hidden layer\n",
        "print(\"The w for the hidden layer are: \",w_hidden)\n",
        "print(\"The b of the hidden layer are: \",b_hidden,\"\\n\")\n",
        "\n",
        "w_output = torch.tensor([[1.0, 1.0, -2.0]]) #The weights of the neurons in the output layer\n",
        "b_output = torch.tensor([-0.5]) #The weight of the bias in the output layer\n",
        "print(\"The w for the output layer are: \",w_output)\n",
        "print(\"The b of the output layer are: \",b_output,\"\\n\")\n",
        "\n",
        "model.set_weights(w_hidden,b_hidden,'hidden')\n",
        "model.set_weights(w_output,b_output,'output')\n",
        "\n",
        "# xor input natrix ant target matrix\n",
        "x = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
        "t = torch.tensor([[0.0], [1.0], [1.0], [0.0]])\n",
        "\n",
        "loss=Loss(x,t) #sum of squares of the error\n",
        "print(\"The value of loss is: \",loss,\"\\n\")\n",
        "\n",
        "# Concatenate input and target tensors\n",
        "xor_table = torch.cat((x.int(),  model(x).int()), dim=1)\n",
        "\n",
        "# Convert to a Pandas DataFrame for a nice tabular representation\n",
        "df = pd.DataFrame(xor_table.numpy(), columns=['Input1', 'Input2', 'Output (XOR)'])\n",
        "\n",
        "# Print the truth table\n",
        "print(\"Truth table:\\n\", df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02gzINu1sXS3",
        "outputId": "65100537-7da5-4682-ec71-ccaf4de1b479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The w for the hidden layer are:  tensor([[1., 1.]])\n",
            "The b of the hidden layer are:  tensor([-1.5000]) \n",
            "\n",
            "The w for the output layer are:  tensor([[ 1.,  1., -2.]])\n",
            "The b of the output layer are:  tensor([-0.5000]) \n",
            "\n",
            "The value of loss is:  tensor(0., grad_fn=<SumBackward0>) \n",
            "\n",
            "Truth table:\n",
            "    Input1  Input2  Output (XOR)\n",
            "0       0       0             0\n",
            "1       0       1             1\n",
            "2       1       0             1\n",
            "3       1       1             0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# מספר הנוירונים בשכבה הנסתרת\n",
        "k = 2\n",
        "model = Network(k, bypass=False)\n",
        "\n",
        "# משקולות והטיות לשכבה הנסתרת\n",
        "w_hidden = torch.tensor([[20.0, 20.0], [-20.0, -20.0]])\n",
        "b_hidden = torch.tensor([-10, 30.0])\n",
        "\n",
        "# משקולות והטיות לשכבת הפלט\n",
        "w_output = torch.tensor([[20.0, 20.0]])\n",
        "b_output = torch.tensor([-30.0])\n",
        "\n",
        "# עדכון המשקולות במודל\n",
        "model.set_weights(w_hidden, b_hidden, 'hidden')\n",
        "model.set_weights(w_output, b_output, 'output')\n",
        "\n",
        "# הקלט והמטרות\n",
        "x = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])  # מטריצת קלט\n",
        "t = torch.tensor([[0.0], [1.0], [1.0], [0.0]])                      # מטריצת המטרות\n",
        "\n",
        "# חישוב השגיאה\n",
        "loss = Loss(x, t)\n",
        "print(\"The value of loss is: \", loss, \"\\n\")\n",
        "\n",
        "# יצירת טבלת אמת\n",
        "xor_table = torch.cat((x.int(), model(x).int()), dim=1)\n",
        "df = pd.DataFrame(xor_table.numpy(), columns=['Input1', 'Input2', 'Output (XOR)'])\n",
        "print(\"Truth table:\\n\", df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc_4_JByDngt",
        "outputId": "fd507c0e-2694-4830-ccc2-c31f17bb34d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of loss is:  tensor(0., grad_fn=<SumBackward0>) \n",
            "\n",
            "Truth table:\n",
            "    Input1  Input2  Output (XOR)\n",
            "0       0       0             0\n",
            "1       0       1             1\n",
            "2       1       0             1\n",
            "3       1       1             0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# מספר הנוירונים בשכבה הנסתרת\n",
        "k = 4\n",
        "model = Network(k, bypass=False)\n",
        "\n",
        "# משקולות והטיות לשכבה הנסתרת\n",
        "w_hidden = torch.tensor([[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0]])\n",
        "b_hidden = torch.tensor([0.5, -0.5, -0.5, -1.5])\n",
        "\n",
        "# משקולות והטיות לשכבת הפלט\n",
        "w_output = torch.tensor([[0.0, 1.0, 1.0, 0.0]])  # גודל 1x4\n",
        "b_output = torch.tensor([-0.5])                   # גודל 1\n",
        "\n",
        "# עדכון המשקולות במודל\n",
        "model.set_weights(w_hidden, b_hidden, 'hidden')\n",
        "model.set_weights(w_output, b_output, 'output')\n",
        "\n",
        "# הקלט והמטרות\n",
        "x = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])  # מטריצת הקלט\n",
        "t = torch.tensor([[0.0], [1.0], [1.0], [0.0]])                      # מטריצת המטרות\n",
        "\n",
        "# חישוב השגיאה\n",
        "loss = Loss(x, t)\n",
        "print(\"The value of loss is: \", loss, \"\\n\")\n",
        "\n",
        "# יצירת טבלת אמת\n",
        "xor_table = torch.cat((x.int(), model(x).int()), dim=1)\n",
        "df = pd.DataFrame(xor_table.numpy(), columns=['Input1', 'Input2', 'Output (XOR)'])\n",
        "print(\"Truth table:\\n\", df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nU2JBs6NWmz",
        "outputId": "2b437d53-e71f-40fc-cf5d-1761af0c9a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of loss is:  tensor(0., grad_fn=<SumBackward0>) \n",
            "\n",
            "Truth table:\n",
            "    Input1  Input2  Output (XOR)\n",
            "0       0       0             0\n",
            "1       0       1             1\n",
            "2       1       0             1\n",
            "3       1       1             0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "From the experiments with:\n",
        "K=1,K=2 and K=4\n",
        "it was observed for K=1, a bypass was necessary to compensate for the simplicity of the network and correctly implement the XOR since XOR is a non-linear function, and a single neuron in the hidden layer is insufficient to model it without additional pathways.\n",
        "for k=2 A bypass was not required. XOR is a special function that can be implemented with two neurons in the hidden layer, making\n",
        "K=2 an optimal configuration for this task.\n",
        "For K=4, the network performed correctly, but the additional neurons were redundant for XOR, demonstrating that increased complexity does not always lead to better performance in simple tasks. The results highlight the importance of balancing network complexity with task requirements to optimize performance and resource efficiency."
      ],
      "metadata": {
        "id": "W7HuYEbpP-7_"
      }
    }
  ]
}
